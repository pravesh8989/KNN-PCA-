{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# KNN & PCA"
      ],
      "metadata": {
        "id": "KwEHt6HPy82G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?**\n",
        "\n",
        "Ans. K-Nearest Neighbors (KNN) is a supervised, non-parametric, instance-based learning algorithm. It makes predictions based on the K closest data points in the feature space using a distance metric.\n",
        "\n",
        "Classification:\n",
        "\n",
        "* The class is decided by majority voting among the K nearest neighbors.\n",
        "\n",
        "* Example: If most neighbors belong to class A, the new point is classified as A.\n",
        "\n",
        "Regression:\n",
        "\n",
        "* The prediction is the average (or weighted average) of the target values of the K nearest neighbors.\n",
        "\n",
        "**Question 2: What is the Curse of Dimensionality and how does it affect KNN performance?**\n",
        "\n",
        "Ans.The Curse of Dimensionality refers to problems that arise when the number of features increases.\n",
        "\n",
        "Effect on KNN:\n",
        "\n",
        "* Distances between points become very similar\n",
        "\n",
        "* Nearest neighbors are no longer meaningful\n",
        "\n",
        "* Model accuracy degrades\n",
        "\n",
        "* Computational cost increases\n",
        "\n",
        "KNN relies heavily on distance calculations, so high-dimensional data significantly reduces its effectiveness. Dimensionality reduction techniques like PCA help mitigate this issue.\n",
        "\n",
        "**Question 3: What is Principal Component Analysis (PCA)? How is it different from feature selection?**\n",
        "\n",
        "Ans.PCA is an unsupervised dimensionality reduction technique that transforms original features into a smaller set of uncorrelated variables called principal components while retaining maximum variance.\n",
        "\n",
        "PCA:\n",
        "* creates new features\n",
        "* unsupervised\n",
        "* removes correlation\n",
        "* maximize variance\n",
        "\n",
        "Feature Selection:\n",
        "* select exist features\n",
        "* often supervised\n",
        "* keeps original meaning\n",
        "* maximize relevance\n",
        "\n",
        "**Question 4: What are eigenvalues and eigenvectors in PCA, and why are they important?**\n",
        "\n",
        "Ans.Eigenvectors define the direction of principal components.\n",
        "\n",
        "Eigenvalues represent the amount of variance captured by each component.\n",
        "\n",
        "Importance:\n",
        "\n",
        "* Eigenvectors determine the new feature space\n",
        "\n",
        "* Eigenvalues help rank components\n",
        "\n",
        "* Higher eigenvalue = more information retained\n",
        "\n",
        "**Question 5: How do KNN and PCA complement each other when applied in a single pipeline?**\n",
        "\n",
        "Ans. PCA reduces dimensionality and removes noise, while KNN benefits from:\n",
        "\n",
        "* Faster distance computation\n",
        "\n",
        "* Improved accuracy\n",
        "\n",
        "* Reduced curse of dimensionality\n",
        "\n",
        "* Together, PCA + KNN form an efficient pipeline for high-dimensional datasets."
      ],
      "metadata": {
        "id": "uCW7Q9tPzBxw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRuztAL4yviF",
        "outputId": "6a7e5940-6951-4f94-a96b-da2b8896625b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----Accuracy Score with Scaling------\n",
            "0.9722222222222222\n",
            "-----Accuracy Score without scaling-------\n",
            "0.6388888888888888\n"
          ]
        }
      ],
      "source": [
        "#Q6.Train a KNN Classifier on the Wine dataset with and without feature\n",
        "# scaling. Compare model accuracy in both cases?\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X,y = load_wine(return_X_y=True)\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=1)\n",
        "knn = KNeighborsClassifier(n_neighbors=6)\n",
        "\n",
        "#with scaling\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "#train data needs fit and transform both\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "#test data requires only transform to prevent data leakage\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "knn.fit(X_train_scaled,y_train)\n",
        "y_pred_scaled = knn.predict(X_test_scaled)\n",
        "print('-----Accuracy Score with Scaling------')\n",
        "print(accuracy_score(y_test,y_pred_scaled))\n",
        "\n",
        "\n",
        "#without scaling\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=6)\n",
        "knn.fit(X_train,y_train)\n",
        "y_pred = knn.predict(X_test)\n",
        "print('-----Accuracy Score without scaling-------')\n",
        "print(accuracy_score(y_test,y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q7 Train a PCA model on the Wine dataset and print the explained variance\n",
        "#ratio of each principal component.\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X,y = load_wine(return_X_y=True)\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=1)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "pca = PCA()\n",
        "pca.fit(X_train_scaled)\n",
        "print('----------------Variance Ration Explained-----------')\n",
        "print(pca.explained_variance_ratio_[0:4]) #first four"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Epv3ARyA4NpL",
        "outputId": "e4d5eced-6e47-4cc1-8f7b-2632b07f3761"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------Variance Ration Explained-----------\n",
            "[0.35684314 0.19825228 0.11659894 0.07517421]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q8 Train a KNN Classifier on the PCA-transformed dataset (retain top 2\n",
        "# components). Compare the accuracy with the original dataset.\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X,y = load_wine(return_X_y=True)\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=1)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "#PCA with 2 components\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "#model\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_pca,y_train)\n",
        "y_pred = knn.predict(X_test_pca)\n",
        "print(f'Accuracy with PCA(2 components) : {accuracy_score(y_test,y_pred)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kK349jt_9f4y",
        "outputId": "1f297f02-93a8-4420-8900-edc65b4ec55a"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with PCA(2 components) : 0.9444444444444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q9 Train a KNN Classifier with different distance metrics (euclidean,\n",
        "# manhattan) on the scaled Wine dataset and compare the results.\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X,y=load_wine(return_X_y=True)\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=1)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_euclidean = KNeighborsClassifier(metric='euclidean')\n",
        "knn_manhattan = KNeighborsClassifier(metric='manhattan')\n",
        "\n",
        "knn_euclidean.fit(X_train_scaled,y_train)\n",
        "knn_manhattan.fit(X_train_scaled,y_train)\n",
        "\n",
        "print('Euclidean Accucarcy :', accuracy_score(y_test,knn_euclidean.predict(X_test_scaled)))\n",
        "print('Manhattan Accucarcy :', accuracy_score(y_test,knn_manhattan.predict(X_test_scaled)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXEGimh4CWGf",
        "outputId": "ad3e3a6a-b1d7-42f8-c8f5-c8a73af752c3"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Euclidean Accucarcy : 0.9722222222222222\n",
            "Manhattan Accucarcy : 0.9722222222222222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q10**\n",
        "\n",
        "Ans. 1. Use PCA\n",
        "\n",
        "* Reduce thousands of gene features\n",
        "\n",
        "* Remove noise and correlation\n",
        "\n",
        " 2 . Choose Components\n",
        "\n",
        "* Use cumulative explained variance (e.g., 95%)\n",
        "\n",
        " 3 . Apply KNN\n",
        "\n",
        "* Train on PCA-transformed data\n",
        "\n",
        "* Choose optimal K using cross-validation\n",
        "\n",
        " 4 . Evaluation\n",
        "\n",
        "* Accuracy\n",
        "\n",
        "* ROC-AUC\n",
        "\n",
        "* Cross-validation score\n",
        "\n",
        " 5 . Business / Medical Justification\n",
        "\n",
        "* Reduces overfitting\n",
        "\n",
        "* Improves generalization\n",
        "\n",
        "* Faster computation\n",
        "\n",
        "* More reliable diagnosis"
      ],
      "metadata": {
        "id": "3uBas5rVGuKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X,y=load_wine(return_X_y=True)\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=1)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "pca = PCA(n_components=0.95)\n",
        "X_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "scores = cross_val_score(knn, X_pca, y_train, cv=5, scoring='accuracy')\n",
        "print(\"Mean CV Accuracy:\", scores.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ghilo8A4EpeY",
        "outputId": "21e5b134-8b5d-4f78-9fe6-b55f46907464"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean CV Accuracy: 0.9719211822660098\n"
          ]
        }
      ]
    }
  ]
}